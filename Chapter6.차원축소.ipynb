{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 단어 임베딩\n",
    "\n",
    "## Intro\n",
    "\n",
    "* 사람의 언어는 불연속적인 형태의 단어로 구성\n",
    "    * 각 단어의 의미는 서로 연관성이 있을 수도 있다.\n",
    "    * 형태가 다르면 컴퓨터가 겉의 형태로만 연관성 파악하기 어렵다.\n",
    "    \n",
    "* 자연어 처리에서 단어,문장,문서를 벡터로 나타낸다.\n",
    "    * 자연어의 형태와 벡터로 변환 가능한 함수 또는 맵핑 테이블 제작하는 과정이 중요하다.\n",
    "    \n",
    "* Review: 코퍼스로부터 단어의 특징 feature를 추출해서 벡터로 만들었다.\n",
    "    * 차원의 저주로 결과물은 여전히 희소 벡터이다. 같은 데이터 표현시 가능한 낮은 차원으로 표현할수록 쉽게 모델링하고 학습 가능하다. \n",
    "    * 희소 벡터로 나타내기 보다 덴스(dense) 벡터로 표현하는 게 훨씬 좋다.\n",
    "* 단어 임베딩 : 단어를 컴퓨터가 이해하고 처리하기 쉬운 형태로 변화하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원축소\n",
    "\n",
    "* 높은 차원에서 데이터 표현하면 희소성 문제가 많이 발생한다. 같은 정보를 표현할 때 더 낮은 차원을 사용하자.\n",
    "* 더 작은 차원으로 효율적으로 정보 표현하는 차원 축소의 이유와 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주성분 분석\n",
    "* 대표적인 차원 축소 방법\n",
    "* 고차원의 데이터를 더 낮은 차원으로 표현 가능\n",
    "* 특잇값 분해(SVD)를 통해 주성분을 분석할 수 있다.\n",
    "* 축소를 위한 주성분 조건\n",
    "    * 고차원에서 주어진 데이터들을 임의의 주성분 고차원 평면(초평면, hyperplane)에 투사했을 때 투사점들 사이가 최대한 멀어져야 한다.\n",
    "    * 투사점들의 분산이 최대가 되도록 한다.\n",
    "    * 고차원 평면으로 투사할 때 원래 벡터와 고차원 평면상의 투사된 거리가 최소가 되어야 한다.\n",
    "    \n",
    "* 고차원의 데이터를 더 낮은 차원으로 효과적으로 압축 가능\n",
    "* 실제 데이터(점)의 위치와 고차원 평면에 투사된 점의 거리가 생긴다.\n",
    "* 정보의 손실이 불가피하다.  \n",
    "( $ \\because$ 주성분은 직선 또는 평면)\n",
    "* 너무 많은 정보의 손실은 효율적인 정보 학습과 복구가 어렵다.\n",
    "    * 고차원 정보를 지나치게 저차원으로 축소해서 표현하기 어렵다.\n",
    "    * 데이터가 비선형적으로 구성될수록 더욱 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매니폴드 가설\n",
    "<img src=\"./asset/chap06/06-02-04.png\" width=\"300\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 고차원에 존재하는 데이터들은 실제로 해당 데이터를 아우르는 낮은 차원의 다양체(manifold)역시 존재한다.(manifold hypotheses)\n",
    "* 3차원 공간에 분포한 2차원 매니폴드를 2차원 평면에 데이터 포인트를 매핑하면, 주성분 분석에서 생기는 손실을 최소화할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./asset/chap06/06-02-05.png\" width=\"500\" height=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 매니폴드 가설의 흥미로운 특징\n",
    "    * 고차원상에서 가까운 데이터 포인트들도 매니폴드를 저차원 공간으로 맵핑하면 거리가 멀어질 수 있다.\n",
    "    * 저차원의 공간상에서 가까운 점끼리는 실제로 비슷한 특징(feature)를 가진다.\n",
    "    * 저차원의 각 공간의 차원축은 고차원에서 비선형적으로 표현되고 데이터의 특징을 각각 표현한다.\n",
    "    \n",
    "* MNIST 데이터를 2차원의 숨겨진 저차원에 표현 가정\n",
    "    * 각 샘플은 2차원 공간에서는 사람이 인지할 수 있는 위치 및 관계.(3, 0)\n",
    "    * 784차원의 고차원 공간에서는 이 위치와 관계는 달라질 것이다.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./asset/chap06/06-02-06.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝이 잘 동작한 이유\n",
    "* 딥러닝의 차원 축소 수행 과정 : 데이터가 존재하는 고차원상에서 매니폴드를 찾는 과정\n",
    "    * 주성분 분석(PCA) 같은 선형적인 방식보다 비선형적인 방식으로 차원 축소를 수행한다.\n",
    "    * 해당 문제를 가장 잘 해결하려는 매니폴드를 자연스럽게 찾아낸다. 그래서 성공적으로 동작한다.\n",
    "<img src=\"./asset/chap06/06-02-02.png\" width=\"500\" height=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오토인코더\n",
    "* 아래와 같은 구조를 가진 딥러닝 모델\n",
    "    * 고차원 샘플 벡터를 입력으로 받아 매니폴드 찾기\n",
    "    * 저차원으로 축소하는 인코더를 거쳐 병목 구간에서의 숨겨진 벡터로 표현\n",
    "    * 디코더 : 저차원 벡터를 받아 다시 원래 입력 샘플이 존재하던 고차원으로 데이터 복원\n",
    "    * 복원된 데이터는 고차원 상의 매니폴드 위에 위치\n",
    "<img src=\"./asset/chap06/06-02-08.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 복원 과정에서 병목의 차원이 매우 낮다.\n",
    "* 복원에 필요한 정보만 남기고 필요 없는 정보는 버려야 한다. 복원에 필요없는 정보부터 없어진다.(정보량이 낮은 것부터 제거)  \n",
    "$\\therefore$ 복원된 데이터와 실제 입력 데이터 사이의 차이를 최소화하도록 손실 함수 구성\n",
    "* 고차원(3차원)에서 저차원(2차원)으로 투사할 때의 정보 손실(복원 오류)\n",
    "\n",
    "<img src=\"./asset/chap06/06-02-10.png\" width=\"500\" height=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저차원으로 데이터를 표현하면서 손실이 발생하므로, 훈련이 완료된 모델도 복원된 데이터는 실제 데이터와 다를 수 있다.\n",
    "* TF-IDF 등으로 구한 희소 단어 특징 벡터를 입력으로 넣고 같은 출력값나오도록 훈련하면, 오토인코더로 병목 계층 결과치를 덴스 단어 임베딩 벡터로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 흔한 오해1\n",
    "* 사전 훈련된 임베딩 벡터\n",
    "    * word2vec을 통해 얻은 단어 임베딩 벡터가 훌륭하게 단어의 특징을 잘 반영하지만 모든 문제를 해결하는 최적의 벡터 임베딩은 아니다.\n",
    "    * 텍스트 분류나 기계번역을 위한 목적 함수는 word2vec과 다른 형태로 존재한다.\n",
    "    * 다른 목적 함수를 통해 훈련한 임베딩 벡터는 원래의 목적에 맞지 않을 수 있다.\n",
    "        * 행복(긍정/부정 분류 문제 vs 기계번역 문제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 없이 신경망 훈련하기\n",
    "\n",
    "* word2vec을 통한 변환 없이도 문제의 특징에 맞는 단어 임베딩 벡터를 구할 수 있다.\n",
    "* 임베딩 계층(embedding layer) 레이어 아키텍쳐 제공(딥러닝 프레임워크)\n",
    "    * 편차(bias)가 없는 선형 계층과 같은 형태를 가진다.\n",
    "    \n",
    "$$ y = emb(x) = Wx,\\\\ where\\ W\\in \\mathbf{R}^{d\\times|V|} and\\ |V|\\ is\\ size\\ of\\ vocabulary$$\n",
    "\n",
    "* 임베딩 계층의 동작 개념 : 0과 곱해지는 부분은 무시될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./asset/chap06/06-03-01.png\" width=\"500\" height=\"300\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최종적으로 모델에서 구한 손실값으로 역전파 및 경사하강법 수행\n",
    "    * 자동적으로 임베딩 계층의 가중치 $W$의 값을 구할 수 있다.\n",
    "* 실제 구현시, 큰 임베딩 계층 가중치와 원핫 인코딩 곱하는 것은 매우 비효율적이다.\n",
    "* 단순히 테이블에서 검색(lookup) 작업 수행\n",
    "* 따라서 단어를 나타낼 때 (임베딩 계층의 입력으로) 원핫 벡터를 굳이 넘겨줄 필요 없이, 1이 존재하는 단어의 인덱스 정숫값만 입력으로 넘겨주면 임베딩 벡터를 얻을 수 있습니다.\n",
    "* 임베딩 계층과의 계산상의 효율성과는 별개로, 원핫 인코딩 벡터를 표현할 때 1이 존재하는 단어의 인덱스 정숫값만 있으면 된다는 것은 자명합니다. 애초에 원핫 인코딩은 n개의 카테고리를 나타내기 위한 방법이기 때문입니다. n차원의 원핫 인코딩 벡터에서 1은 한 개이며, $n-1$ 개의 0으로 채워집니다.\n",
    "* 추후 다룰 텍스트 분류나 기계번역 장에서 구현한 내용을 살펴보면, word2vec을 사용하여 단어를 임베딩 벡터로 변환한 후 신경망에 직접 넣는 대신, 앞에서 언급한 것처럼 임베딩 계층을 사용하여 원핫 인코딩 벡터를 입력으로 넣도록 구현했음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래도 word2vec 적용하는 경우\n",
    "* 그래도 사전 훈련된 단어 임베딩 벡터 적용을 고려해볼 만한 몇몇 상황이 있습니다.\n",
    "    * 준비된 코퍼스의 양이 너무 적고, 이때 외부로부터 많은 양의 말뭉치를 통해 미리 훈련한 단어 임베딩 벡터를 구한다는 특수한 경우를 가정해볼 수 있습니다.\n",
    "    * 한편으로는 기본 정석대로 먼저 베이스라인(baseline) 모델을 만든 후, 성능을 끌어올리기 위한 여러 가지 방법들을 시도할 때 사전 훈련된 단어 임베딩 벡터의 사용을 고려해볼 수도 있습니다.\n",
    "    * 또는 이 책의 마지막 장에서 다룰 전이학습(transfer learning)에서 살펴볼 고도화된 언어 모델을 통해 사전 훈련하여 접근해볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "* 토마스 미콜라프 제시한 개념\n",
    "* 단어를 임베딩하는 두 가지 방법으로 **CBOW, Skip-gram** 제시\n",
    "    * 공통된 가정 : 함께 등장하는 단어가 비슷할수록 비슷한 벡터 값을 가질 것이다.\n",
    "    * 윈도우 크기가 주어지면 특정 단어를 기준으로 윈도우 내 주변 단어들을 사용하여 단어 임베딩 학습\n",
    "    * 윈도우 내에서의 위치는 고려하지 않는다. \n",
    "    * 단어의 위치 정보를 무시하지도 않는다. 윈도우 자체가 단어의 위치 정보를 내포하기 때문이다.\n",
    "    * 문장 내 단어의 위치에 따라서 윈도우에 포함되는 단어는 달라진다.\n",
    "<img src=\"./asset/chap06/06-04-01.png\" width=\"500\" height=\"300\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 개요: CBOW 방식과 skip-gram 방식\n",
    "* CBOW(continuous bag of words) 방식 학습원리\n",
    "    * 신경망이 주변에 나타난 단어들을 원핫 인코딩된 벡터로 입력받아 해당 단어를 예측한다.\n",
    "* Skip-gram 방식 학습 원리\n",
    "    * 대상 단어를 원핫 인코딩된 벡터로 입력받아 주변 나타나는 단어 예측하는 네트워크 구성해 단어 임베딩 벡터 학습한다.\n",
    "    * CBOW보다 성능이 뛰어난 것으로 알려지고 널리 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상세 훈련 방식 : Skip-gram 방식 학습 과정\n",
    "1. MLE를 통해 다음 수식의 argmax내 수식을 최대로 하는 파라미터 $\\theta$를 찾는다.\n",
    "2. 이를 통해 $w_t$가 주어질 때, 앞 뒤 $n$개의 단어를 예측하도록 훈련한다.($w_{t- {n\\over2}}, \\cdots, w_{t+{n\\over2}}$) 윈도우의 크기는 $n$이다.\n",
    "\n",
    "$$\\hat{\\theta} = \\underset {\\theta} {argmax} \\sum_{t=1}^T\\left(\\sum_{i=1}^n \\log P(w_{t-i}|w_t;\\theta)+ \\sum_{i=1}^n \\log P(w_{t+i}|w_t;\\theta)\\right)$$\n",
    "\n",
    "3. 가중치 $W$를 구하고, softmax를 이용해 복원 단어를 출력한다.\n",
    "    * 임베딩 계층을 통해 원핫 인코딩 벡터를 덴스 벡터인 단어 임베딩 벡터로 변환한다.\n",
    "\n",
    "$$\\hat{y} = \\underset{y \\in \\mathcal{Y}} {argmax}\\ softmax(W'Wx) \\\\ where\\ W' \\in \\mathbf{R}^{|V| \\times d}\\ and \\ x \\in \\{0,1\\}^{|V|}$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 수식을 그림으로 나타내면 다음과 같이 간단한 구조이다. softmax 계층은 출력층이고 네트워크 내부에 시그모이드 같은 비선형 함수가 없다.\n",
    "<img src=\"./asset/chap06/skip-gram.png\" width=\"700\" height=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞서 설명한 임베딩 계층의 동작 원리 처럼 $W$의 각 행이 skip-gram을 통해 얻은 단어 x에 대한 단어 임베딩 벡터가 된다.\n",
    "* Skip-gram을 통해 얻은 단어 임베딩 벡터를 3차원 공간에 PCA로 표현한 그림은 다음과 같다.\n",
    "<img src=\"./asset/chap06/06-04-02.png\" width=\"500\" height=\"300\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전의 특징벡터보다 훨씬 정교하게 단어 임베딩 벡터가 구성된다.\n",
    "* 차원 축소된 공간에 단어들이 빽빽하게 분포하며, 동시에 비슷한 단어들끼리 가까이 분포되어 있음을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
